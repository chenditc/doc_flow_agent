import unittest
import asyncio
from unittest.mock import patch, MagicMock, mock_open
import json
import os
import uuid

from tools.python_executor_tool import PythonExecutorTool


class TestPythonExecutorToolDirect(unittest.TestCase):
    """Direct unit tests for PythonExecutorTool functionality"""
    
    def setUp(self):
        self.mock_llm_tool = MagicMock()
        self.tool = PythonExecutorTool(llm_tool=self.mock_llm_tool)

    def test_tool_initialization(self):
        """Test that the tool initializes correctly"""
        self.assertEqual(self.tool.tool_id, "PYTHON_EXECUTOR")
        self.assertEqual(self.tool.llm_tool, self.mock_llm_tool)

    def test_create_python_code_tool_schema(self):
        """Test the tool schema creation"""
        schema = self.tool._create_python_code_tool_schema()
        
        self.assertEqual(schema["type"], "function")
        self.assertEqual(schema["function"]["name"], "generate_python_code")
        self.assertIn("code", schema["function"]["parameters"]["properties"])
        self.assertEqual(
            schema["function"]["parameters"]["required"], 
            ["code"]
        )

    def test_extract_python_code_from_tool_call_response(self):
        """Test extracting Python code from tool call response"""
        response = {
            "tool_calls": [{
                "name": "generate_python_code",
                "arguments": {
                    "code": "def process_step(context):\n    return context['a'] + context['b']"
                }
            }]
        }
        
        code = self.tool._extract_python_code_from_response(response)
        self.assertIn("def process_step(context)", code)
        self.assertIn("return context['a'] + context['b']", code)

    def test_extract_python_code_from_string_response_raises(self):
        """String responses are not supported (must be tool call)."""
        response = "def process_step(context):\n    return context['x'] * 2"
        with self.assertRaises(ValueError):
            self.tool._extract_python_code_from_response(response)

    def test_extract_python_code_no_tool_calls(self):
        """Test extracting Python code when no tool calls present"""
        response = {"some_key": "some_value"}
        
        with self.assertRaises(ValueError) as context:
            self.tool._extract_python_code_from_response(response)
        self.assertIn("No tool call in LLM response", str(context.exception))

    def test_extract_python_code_wrong_tool_call(self):
        """Test error when wrong tool call name is used"""
        response = {
            "tool_calls": [{
                "name": "wrong_tool",
                "arguments": {"code": "some code"}
            }]
        }
        
        with self.assertRaises(ValueError) as context:
            self.tool._extract_python_code_from_response(response)
        
        self.assertIn("Unexpected tool call: wrong_tool", str(context.exception))

    def test_extract_python_code_no_code_in_args(self):
        """Test error when no code in arguments"""
        response = {
            "tool_calls": [{
                "name": "generate_python_code",
                "arguments": {}
            }]
        }
        
        with self.assertRaises(ValueError) as context:
            self.tool._extract_python_code_from_response(response)
        
        self.assertIn("No Python code generated by LLM", str(context.exception))

    @patch("subprocess.run")
    @patch("uuid.uuid4")
    @patch("os.remove")
    @patch("builtins.open", new_callable=mock_open)
    @patch("os.path.exists", return_value=True)
    def test_execute_with_tool_schema_success(self, mock_exists, mock_open_func, mock_remove, mock_uuid, mock_subprocess_run):
        """Test successful execution using tool schema"""
        async def run_test():
            # Setup mocks
            mock_uuid.return_value = uuid.UUID("12345678-1234-5678-1234-567812345678")
            run_id = "12345678-1234-5678-1234-567812345678"
            
            # Mock LLM response with tool call
            async def mock_llm_execute(params, **kwargs):
                return {
                    "tool_calls": [{
                        "name": "generate_python_code",
                        "arguments": {
                            "code": "def process_step(context):\n    return {'result': context['a'] + context['b']}"
                        }
                    }]
                }
            
            self.mock_llm_tool.execute = mock_llm_execute

            # Mock subprocess
            mock_process = MagicMock()
            mock_process.stdout = "stdout message"
            mock_process.stderr = "stderr message"
            mock_subprocess_run.return_value = mock_process

            # Mock file operations
            result_data = {"return_value": {"result": 15}, "exception": None}
            mock_open_func.side_effect = [
                mock_open().return_value,  # context file write
                mock_open().return_value,  # code file write  
                mock_open(read_data=json.dumps(result_data)).return_value,  # result file read
            ]

            # Execute
            params = {
                "task_description": "Add two numbers from context",
                "related_context_content": {"a": 5, "b": 10},
            }
            result = await self.tool.execute(params)

            # Verify subprocess call
            expected_context_file = f"/tmp/context_{run_id}.json"
            expected_code_file = f"/tmp/code_{run_id}.py" 
            expected_output_file = f"/tmp/result_{run_id}.json"
            
            # Allow either 'python' or sys.executable for portability
            called_args, called_kwargs = mock_subprocess_run.call_args
            assert called_kwargs == {"capture_output": True, "text": True, "check": False}
            assert called_args[0][1:] == [
                "tools/executor_runner.py",
                "--code-file", expected_code_file,
                "--context-file", expected_context_file,
                "--output-file", expected_output_file,
            ]

            # Verify result
            self.assertEqual(result["return_value"], {"result": 15})
            self.assertEqual(result["stdout"], "stdout message")
            self.assertEqual(result["stderr"], "stderr message")
            self.assertIsNone(result["exception"])
            self.assertIn("def process_step(context)", result["python_code"])
            
            # Verify cleanup
            self.assertEqual(mock_remove.call_count, 3)

        asyncio.run(run_test())

    @patch("subprocess.run")
    @patch("uuid.uuid4")
    @patch("os.remove")
    @patch("builtins.open", new_callable=mock_open)
    @patch("os.path.exists", return_value=True)
    def test_execute_retries_on_syntax_error_then_succeeds(self, mock_exists, mock_open_func, mock_remove, mock_uuid, mock_subprocess_run):
        """Test that syntax errors trigger retries until valid code is generated"""

        async def run_test():
            # Recreate tool with limited attempts for clarity
            self.tool = PythonExecutorTool(llm_tool=self.mock_llm_tool, max_generation_attempts=3)

            mock_uuid.return_value = uuid.UUID("12345678-1234-5678-1234-567812345678")
            run_id = "12345678-1234-5678-1234-567812345678"

            responses = [
                {
                    "tool_calls": [{
                        "name": "generate_python_code",
                        "arguments": {
                            "code": "def process_step(context)\n    return context['a']"  # Missing colon
                        }
                    }]
                },
                {
                    "tool_calls": [{
                        "name": "generate_python_code",
                        "arguments": {
                            "code": "def process_step(context):\n    return {'result': context['a'] + context['b']}"
                        }
                    }]
                }
            ]

            attempts = {"count": 0}

            # Simulate LLMTool retry loop driven by validators/max_retries
            async def mock_llm_execute(params, **kwargs):
                max_retries = int(kwargs.get("max_retries", 0) or 0)
                validators = kwargs.get("validators", []) or []
                last_exc = None

                for _ in range(max_retries + 1):
                    attempts["count"] += 1
                    resp = responses.pop(0)
                    try:
                        for v in validators:
                            v(resp)
                        return resp
                    except Exception as e:  # noqa: BLE001 - test harness
                        last_exc = e
                        continue

                raise last_exc  # type: ignore[misc]

            self.mock_llm_tool.execute = mock_llm_execute

            mock_process = MagicMock()
            mock_process.stdout = "stdout message"
            mock_process.stderr = "stderr message"
            mock_subprocess_run.return_value = mock_process

            result_data = {"return_value": {"result": 3}, "exception": None}
            mock_open_func.side_effect = [
                mock_open().return_value,  # context file write
                mock_open().return_value,  # code file write
                mock_open(read_data=json.dumps(result_data)).return_value,  # result file read
            ]

            params = {
                "task_description": "Add two numbers from context",
                "related_context_content": {"a": 1, "b": 2},
            }

            result = await self.tool.execute(params)

            self.assertEqual(result["return_value"], {"result": 3})
            self.assertIsNone(result["exception"])
            self.assertIn("def process_step(context):", result["python_code"])
            self.assertGreaterEqual(attempts["count"], 2)

            # Ensure subprocess executed only once despite retry
            self.assertEqual(mock_subprocess_run.call_count, 1)

            expected_context_file = f"/tmp/context_{run_id}.json"
            expected_code_file = f"/tmp/code_{run_id}.py"
            expected_output_file = f"/tmp/result_{run_id}.json"

            called_args, called_kwargs = mock_subprocess_run.call_args
            assert called_kwargs == {"capture_output": True, "text": True, "check": False}
            assert called_args[0][1:] == [
                "tools/executor_runner.py",
                "--code-file", expected_code_file,
                "--context-file", expected_context_file,
                "--output-file", expected_output_file,
            ]

        asyncio.run(run_test())

    def test_execute_raises_after_retry_exhaustion(self):
        """Test that a SyntaxError is raised when all retries fail"""

        async def run_test():
            self.tool = PythonExecutorTool(llm_tool=self.mock_llm_tool, max_generation_attempts=2)

            call_counter = {"count": 0}

            responses = [
                {
                    "tool_calls": [{
                        "name": "generate_python_code",
                        "arguments": {
                            "code": "def process_step(context)\n    return context['a']"  # Missing colon keeps failing
                        }
                    }]
                },
                {
                    "tool_calls": [{
                        "name": "generate_python_code",
                        "arguments": {
                            "code": "def process_step(context)\n    return context['a']"  # Still invalid
                        }
                    }]
                },
            ]

            async def mock_llm_execute(params, **kwargs):
                max_retries = int(kwargs.get("max_retries", 0) or 0)
                validators = kwargs.get("validators", []) or []
                last_exc = None

                for _ in range(max_retries + 1):
                    call_counter["count"] += 1
                    resp = responses.pop(0)
                    try:
                        for v in validators:
                            v(resp)
                        return resp
                    except Exception as e:  # noqa: BLE001 - test harness
                        last_exc = e
                        continue

                raise last_exc  # type: ignore[misc]

            self.mock_llm_tool.execute = mock_llm_execute

            params = {
                "task_description": "Add numbers",
                "related_context_content": {"a": 1, "b": 2},
            }

            with self.assertRaises(SyntaxError):
                await self.tool.execute(params)

            self.assertEqual(call_counter["count"], 2)

        asyncio.run(run_test())

    @patch("subprocess.run")
    @patch("uuid.uuid4")
    @patch("os.remove")
    @patch("builtins.open", new_callable=mock_open)
    @patch("os.path.exists")
    def test_execute_output_file_missing(self, mock_exists, mock_open_func, mock_remove, mock_uuid, mock_subprocess_run):
        """Test execution when output file is missing"""
        async def run_test():
            # Setup mocks
            mock_uuid.return_value = uuid.UUID("12345678-1234-5678-1234-567812345678")
            
            # Mock LLM response
            async def mock_llm_execute(params, **kwargs):
                return {
                    "tool_calls": [{
                        "name": "generate_python_code", 
                        "arguments": {"code": "def process_step(context):\n    return 'test'"}
                    }]
                }
            
            self.mock_llm_tool.execute = mock_llm_execute

            # Mock subprocess
            mock_process = MagicMock()
            mock_process.stdout = "stdout message"
            mock_process.stderr = "stderr message" 
            mock_subprocess_run.return_value = mock_process

            # Mock file operations - output file doesn't exist
            def exists_side_effect(path):
                return not path.endswith("result_12345678-1234-5678-1234-567812345678.json")
            
            mock_exists.side_effect = exists_side_effect
            mock_open_func.side_effect = [
                mock_open().return_value,  # context file write
                mock_open().return_value,  # code file write
            ]

            # Execute
            params = {
                "task_description": "Test task",
                "related_context_content": {"test": "data"},
            }
            result = await self.tool.execute(params)

            # Verify result handles missing output file
            self.assertIsNone(result["return_value"])
            self.assertEqual(result["exception"], "Output file not found.")
            self.assertEqual(result["stdout"], "stdout message")
            self.assertEqual(result["stderr"], "stderr message")

        asyncio.run(run_test())

    def test_execute_with_legacy_string_response(self):
        """Legacy string responses are rejected (must be tool call)."""
        async def run_test():
            # Mock LLM to return string instead of tool call
            async def mock_llm_execute(params, **kwargs):
                return "def process_step(context):\n    return 'legacy'"

            self.mock_llm_tool.execute = mock_llm_execute
            with self.assertRaises(ValueError):
                await self.tool.execute(
                    {"task_description": "Legacy test", "related_context_content": {}}
                )

        asyncio.run(run_test())

if __name__ == "__main__":
    unittest.main()
