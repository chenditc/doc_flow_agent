import asyncio
import json
import os
import pytest
from tools.llm_tool import LLMTool


def test_xml_fallback_parsing(monkeypatch):
    """Test that when no native tool_calls returned, fallback XML parsing works.

    We simulate two sequential streaming calls:
      1. Returns no tool_calls and some irrelevant content.
      2. Returns XML-wrapped JSON arguments.
    """
    monkeypatch.setenv("INTEGRATION_TEST_MODE", "MOCK")
    tool = LLMTool()

    # Tool schema similar to real usage
    tools = [
        {
            "type": "function",
            "function": {
                "name": "generate_python_code",
                "description": "Generate Python code for the process_step function",
                "parameters": {
                    "type": "object",
                    "properties": {"code": {"type": "string"}},
                    "required": ["code"],
                },
            },
        }
    ]

    class DummyDelta:
        def __init__(self, content=None, tool_calls=None):
            self.content = content
            self.tool_calls = tool_calls

    class DummyChoice:
        def __init__(self, delta):
            self.delta = delta

    class DummyChunk:
        def __init__(self, content=None, tool_calls=None):
            self.choices = [DummyChoice(DummyDelta(content=content, tool_calls=tool_calls))]

    class FirstStream:
        def __init__(self):
            self._i = 0
        def __aiter__(self):
            return self
        async def __anext__(self):
            if self._i > 0:
                raise StopAsyncIteration
            self._i += 1
            return DummyChunk(content="No tool call here.")

    class SecondStream:
        def __init__(self):
            self._i = 0
        def __aiter__(self):
            return self
        async def __anext__(self):
            if self._i > 0:
                raise StopAsyncIteration
            self._i += 1
            # Provide final XML block
            return DummyChunk(content='<generate_python_code>{"code": "print(\\"hi\\")"}</generate_python_code>')

    call_count = {"n": 0}

    async def fake_create(**kwargs):  # type: ignore
        # First invocation -> first stream, second -> second stream
        call_count["n"] += 1
        if call_count["n"] == 1:
            return FirstStream()
        return SecondStream()

    monkeypatch.setattr(tool.client.chat.completions, "create", fake_create)

    async def run():
        result = await tool.execute({"prompt": "Test fallback", "tools": tools})
        return result

    try:
        result = asyncio.run(run())
    except RuntimeError as e:
        if "asyncio.run() cannot be called" in str(e):
            loop = asyncio.new_event_loop()
            try:
                result = loop.run_until_complete(run())
            finally:
                loop.close()
        else:
            raise

    assert call_count["n"] == 2, "Should have made two LLM calls (original + fallback)"
    assert result["tool_calls"], "Fallback should produce tool_calls"
    assert result["tool_calls"][0]["name"] == "generate_python_code"
    assert result["tool_calls"][0]["arguments"]["code"].startswith("print"), "Parsed code argument missing or incorrect"


def test_public_parse_helper():
    tool = LLMTool()
    tools = [
        {"type": "function", "function": {"name": "foo", "parameters": {"type": "object", "properties": {"a": {"type": "number"}}, "required": ["a"]}}}
    ]
    content = '<foo>{"a": 3}</foo>'
    calls = tool.parse_tool_call_from_content(content, tools)
    assert calls and calls[0]["arguments"]["a"] == 3


@pytest.mark.skipif(os.getenv("INTEGRATION_TEST_MODE", "").lower() != "real", reason="Runs only in REAL integration mode due to real LLM call + long prompt size")
@pytest.mark.asyncio
async def test_long_prompt_real_mode_extraction():
    """Stress test: ensure long prompt with complex embedded XML-like content still yields tool call.

    This replicates a special case where some models fail to return proper tool_calls
    for long, instruction-heavy prompts (like the one seen in test.py stream_real). We
    send a condensed but still sizable prompt plus a tool schema and assert that a
    native tool call OR a fallback-parsed tool call is produced.

    Only executes when INTEGRATION_TEST_MODE=REAL to avoid network usage in MOCK mode.
    """
    from dotenv import load_dotenv
    load_dotenv()
    tool = LLMTool()

    long_prompt = """
You are a Python code generation assistant.
Your task is to write a single Python function named `process_step` that takes one argument: `context: dict`.
This function will be executed to perform following specific task. Import necessary library if you used any.
The context object will contain all the necessary data. The json serialized context object has been attached here for you to understand the input data structure.
The function should return a JSON-serializable value.

<available library>
aiohappyeyeballs
aiohttp
aiosignal
annotated-types
anyio
attrs
azure-core
azure-identity
certifi
cffi
charset-normalizer
click
cryptography
distro
fastapi
frozenlist
genson
h11
httpcore
httptools
httpx
idna
iniconfig
jiter
json_repair
jsonpath-ng
msal
msal-extensions
multidict
openai
packaging
pip
pluggy
ply
propcache
pycparser
pydantic
pydantic_core
PyJWT
pytest
pytest-asyncio
PyYAML
requests
six
sniffio
starlette
tqdm
typing-inspection
typing_extensions
urllib3
uvicorn
uvloop
watchdog
watchfiles
websockets
yarl
</available library>

<Task Description>
è¯·æŒ‰å½“å‰ä»»åŠ¡çš„è¦æ±‚ç”Ÿæˆå›¾ç‰‡ï¼Œå¹¶ä¸‹è½½ä¿å­˜åˆ°æœ¬åœ°ã€‚è¿”å›å›¾ç‰‡ url ä»¥åŠæœ¬åœ°å›¾ç‰‡è·¯å¾„ã€‚
</Task Description>
<Document Guidance>
## è¯¥ API çš„ curl è°ƒç”¨ç¤ºä¾‹

Following instruction is fetched from: https://www.volcengine.com/docs/82379/1541523

ARK_API_KEY å¯ä»¥ä»ç¯å¢ƒå˜é‡è·å–

prompt éœ€è¦å¹²å‡€ï¼ˆä¸åŒ…å«æ— å…³ä¿¡æ¯ï¼‰ï¼Œè¯¦ç»†ï¼ˆå¯¹ç»†èŠ‚æè¿°åˆ°ä½ï¼‰ã€‚

curl -X POST https://ark.cn-beijing.volces.com/api/v3/images/generations \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ARK_API_KEY" \
  -d '{
    "model": "doubao-seedream-4-0-250828",
    "prompt": "æ˜Ÿé™…ç©¿è¶Šï¼Œé»‘æ´ï¼Œé»‘æ´é‡Œå†²å‡ºä¸€è¾†å¿«æ”¯ç¦»ç ´ç¢çš„å¤å¤åˆ—è½¦ï¼ŒæŠ¢è§†è§‰å†²å‡»åŠ›ï¼Œç”µå½±å¤§ç‰‡ï¼Œæœ«æ—¥æ—¢è§†æ„Ÿï¼ŒåŠ¨æ„Ÿï¼Œå¯¹æ¯”è‰²ï¼Œocæ¸²æŸ“ï¼Œå…‰çº¿è¿½è¸ªï¼ŒåŠ¨æ€æ¨¡ç³Šï¼Œæ™¯æ·±ï¼Œè¶…ç°å®ä¸»ä¹‰ï¼Œæ·±è“ï¼Œç”»é¢é€šè¿‡ç»†è…»çš„ä¸°å¯Œçš„è‰²å½©å±‚æ¬¡å¡‘é€ ä¸»ä½“ä¸åœºæ™¯ï¼Œè´¨æ„ŸçœŸå®ï¼Œæš—é»‘é£èƒŒæ™¯çš„å…‰å½±æ•ˆæœè¥é€ å‡ºæ°›å›´ï¼Œæ•´ä½“å…¼å…·è‰ºæœ¯å¹»æƒ³æ„Ÿï¼Œå¤¸å¼ çš„å¹¿è§’é€è§†æ•ˆæœï¼Œè€€å…‰ï¼Œåå°„ï¼Œæè‡´çš„å…‰å½±ï¼Œå¼ºå¼•åŠ›ï¼Œåå™¬",
    "size": "2K",
    "sequential_image_generation": "disabled",
    "stream": false,
    "response_format": "url",
    "watermark": true
}'

Example response:

{
    "model": "doubao-seedream-4-0-250828",
    "created": 1757321139,
    "data": [
        {
            "url": "https://...",
            "size": "3104x1312"
        }
    ],
    "usage": {
        "generated_images": 1,
        "output_tokens": xxx,
        "total_tokens": xxx
    }
}

## Example code

def process_step(context: dict):
    import requests
    import json
    import os 
    import json 
    import time 
    from typing import Any, Dict

    API_ENDPOINT = "https://ark.cn-beijing.volces.com/api/v3/images/generations"
    MODEL_NAME = "doubao-seedream-4-0-250828"


    # ---------- å›¾åƒè®¾è®¡æŒ‡ä»¤ï¼ˆå¦‚æœç”¨æˆ·ä¸Šä¸‹æ–‡ä¸æ˜¯å·²ç»åœ¨æè¿°æµ·æŠ¥ï¼‰ ----------
    design_addendum = (
        "è®¾è®¡ä¸€å¼ ç«–å±å­¦ä¹ åŠ©åŠ›æµ·æŠ¥ï¼Œå¼ºè°ƒæ¸…æ™°æ’ç‰ˆä¸å¯è¯»æ€§ã€‚"
        " ä¸»é¢˜æ˜¯ä¸­è€ƒå€’è®¡æ—¶å¿ƒç†æ”¯æŒã€‚èƒŒæ™¯ä¸ºæŸ”å’Œç±³é»„è‰²æˆ–æµ…å¥¶å’–çº¯è‰²ï¼Œå››è§’åœ†æ¶¦å¡ç‰‡æ„Ÿã€‚"
        " å±…ä¸­æ”¾ç½®ä¸»æ ‡é¢˜ï¼šâ€˜è·ç¦»ä¸­è€ƒæœ€å30å¤©â€™ï¼ˆè¶…å¤§é»‘ä½“æˆ–æ·±æ£•ç²—ä½“ï¼Œé«˜å¯¹æ¯”ï¼Œå¯è¯»æ€§å¼ºï¼‰ã€‚"
        " ä¸»æ ‡é¢˜ä¸‹æ–¹ç•™å‡ºå‘¼å¸æ„Ÿæ”¾å‰¯æ ‡é¢˜ï¼šâ€˜å®¶é•¿è¿™10ç§å¿ƒç†åŠ©åŠ› = å­©å­å°‘èµ°å¼¯è·¯â€™ã€‚"
        " å³ä¸‹è§’å°å­—ä¸å›¾æ ‡ï¼šâ€˜ğŸ“Œ æ”¶è—æ…¢çœ‹ Â· ç¨³ä½æ¯”åˆ†ä¸ç„¦è™‘â€™ã€‚"
        " å·¦ä¸Šè§’ä¸€ä¸ªğŸ“æˆ–ğŸ“˜å›¾æ ‡ï¼ˆç®€æ´æ‰å¹³é£ï¼‰ï¼Œå³ä¸Šè§’è´´è¾¹æœ‰ä¸€æ¡æµ…è‰²ä¾¿åˆ©è´´æ•ˆæœçº¸æ¡ï¼Œä¸Šé¢æ‰‹å†™é£æˆ–ç»†å­—ï¼šâ€˜ç¨³æƒ…ç»ª > æ‹¼æ—¶é•¿â€™ã€‚"
        " ç”»é¢å››å‘¨ç‚¹çŠ¶æµ…ç»¿è‰²æˆ–æµ…è“å°åœ†ç‚¹ä½œè½»ç›ˆè£…é¥°ï¼Œä¿æŒç•™ç™½ã€‚"
        " é£æ ¼ï¼šæ¸…çˆ½ã€èˆ’ç¼“ã€æ•™è‚²æµ·æŠ¥ã€ä¿¡æ¯å±‚çº§æ¸…æ™°ã€ç°ä»£æ‰å¹³åŒ–ã€è½»è´¨æ„Ÿé˜´å½±ã€è‰²å½©å±‚æ¬¡ç»†è…»ã€ä¸è¿‡åº¦èŠ±å“¨ã€‚"
        " çºµå‘æ„å›¾ï¼Œç«–å± 4:3ï¼ˆå®½:é«˜â‰ˆ3:4ï¼‰æ¯”ä¾‹ã€‚é«˜åˆ†è¾¨ç‡ï¼Œå¹²å‡€æ— æ°´å°ï¼Œæ— é¢å¤–æ— å…³å…ƒç´ ã€‚"
        " æ–‡æœ¬éœ€æ¸…æ™°é”åˆ©ï¼Œä¸å˜å½¢ã€‚"
    )

    final_prompt = design_addendum

    api_key = os.getenv("ARK_API_KEY")
    if not api_key:
        return {
            "success": False,
            "error": "Missing ARK_API_KEY in environment",
            "local_path": None,
            "used_size": None,
            "prompt": final_prompt,
            "model": MODEL_NAME,
        }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    last_error = None
    response_json = None
    used_size = None

    # æ¨èçš„å®½é«˜åƒç´ å€¼ï¼š
    # 1:1
    # 2048x2048
    # 4:3
    # 2304x1728
    # 3:4
    # 1728x2304
    # 16:9
    # 2560x1440
    # 9:16
    # 1440x2560
    # 3:2
    # 2496x1664
    # 2:3
    # 1664x2496
    # 21:9
    # 3024x1296
    size = "1728x2304"  # 3:4

    payload = {
        "model": MODEL_NAME,
        "prompt": final_prompt,
        "size": size,
        "sequential_image_generation": "disabled",
        "stream": False,
        "response_format": "url",
        "watermark": False,
    }

    resp = requests.post(API_ENDPOINT, headers=headers, json=payload, timeout=120)

    # æ­£å¸¸ 200 å“åº”
    response_json = resp.json()

    remote_url = response_json["data"][0].get("url")
    if not remote_url:
        return {
            "success": False,
            "error": "No URL returned in API response",
            "local_path": None,
            "used_size": used_size,
            "prompt": final_prompt,
            "model": MODEL_NAME,
        }

    # ä¸‹è½½å›¾ç‰‡
    local_filename = f"generated_poster_{int(time.time())}.png"
    local_path = os.path.abspath(local_filename)

    try:
        img_resp = requests.get(remote_url, timeout=120)
        if img_resp.status_code == 200:
            with open(local_path, "wb") as f:
                f.write(img_resp.content)
        else:
            return {
                "success": False,
                "error": f"Failed to download image: HTTP {img_resp.status_code}",
                "local_path": None,
                "used_size": used_size,
                "prompt": final_prompt,
                "model": MODEL_NAME,
            }
    except Exception as e:
        return {
            "success": False,
            "error": f"Exception downloading image: {e}",
            "local_path": None,
            "used_size": used_size,
            "prompt": final_prompt,
            "model": MODEL_NAME,
        }

    # æˆåŠŸè¿”å›
    meta = {}
    for k in ("created", "usage"):
        if isinstance(response_json, dict) and k in response_json:
            meta[k] = response_json[k]

    return {
        "success": True,
        "local_path": local_path,
        "prompt": final_prompt,
        "model": MODEL_NAME,
        "api_response_meta": meta,
    }
</Document Guidance>

<context object type>dict</context object type>
<Json serialized context object>
{
  "main_title": "ä¸‡èƒ½ç»“æ„é€†è¢­",
  "description": "äº®é»„åº•ï¼Œä¸­å¿ƒå¤§å·æ‰‹å†™ä¸»æ ‡é¢˜ç™½å­—çº¢æè¾¹å¾®å€¾ï¼Œå‘¨å›´ç²‰è“æ¶‚é¸¦äº‘ã€‚å·¦ä¸Šçº¢ä¾¿ç­¾ï¼šæœŸä¸­æœ€å1ä¸ªæœˆï¼›å³ä¸Šè“åœ†æ¡†ï¼šå…ˆæŠ¢ç»“æ„åˆ†ï¼›ä¸‹æ–¹Så½¢ç®­å¤´ä¸‰æ­¥ï¼šâ‘ å®¡é¢˜æŠ“ç‚¹ â‘¡ä¸‡èƒ½ç»“æ„æ­éª¨æ¶ â‘¢å¼€å¤´ç»“å°¾+ç»†èŠ‚å‘¨æ”»ã€‚å·¦ä¸‹æ©™åœˆï¼šå‘Šåˆ«æµæ°´è´¦ã€‚å³ä¸‹ç²‰ç«ç„°+âœ¨ï¼šç¨³æ­¥æåˆ†ã€‚æ•£å¸ƒğŸ“ŒğŸ’¡â¤ï¸ğŸ”¥âœ¨ã€‚æ‰‹ç»˜ä¸‹åˆ’çº¿å¼ºè°ƒâ€œç»“æ„â€ã€‚åº•éƒ¨å°ç­¾ï¼šè¾°è¾°è€å¸ˆã€‚æ•´ä½“é«˜å¯¹æ¯”æ‰‹å¸æ¶‚é¸¦æ„Ÿå†²åˆºæ°›å›´ã€‚",
  "prompt": "ä¸»æ ‡é¢˜ï¼šä¸‡èƒ½ç»“æ„é€†è¢­ å°é¢æè¿°ï¼šäº®é»„åº•ï¼Œä¸­å¿ƒå¤§å·æ‰‹å†™ä¸»æ ‡é¢˜ç™½å­—çº¢æè¾¹å¾®å€¾ï¼Œå‘¨å›´ç²‰è“æ¶‚é¸¦äº‘ã€‚å·¦ä¸Šçº¢ä¾¿ç­¾ï¼šæœŸä¸­æœ€å1ä¸ªæœˆï¼›å³ä¸Šè“åœ†æ¡†ï¼šå…ˆæŠ¢ç»“æ„åˆ†ï¼›ä¸‹æ–¹Så½¢ç®­å¤´ä¸‰æ­¥ï¼šâ‘ å®¡é¢˜æŠ“ç‚¹ â‘¡ä¸‡èƒ½ç»“æ„æ­éª¨æ¶ â‘¢å¼€å¤´ç»“å°¾+ç»†èŠ‚å‘¨æ”»ã€‚å·¦ä¸‹æ©™åœˆï¼šå‘Šåˆ«æµæ°´è´¦ã€‚å³ä¸‹ç²‰ç«ç„°+âœ¨ï¼šç¨³æ­¥æåˆ†ã€‚æ•£å¸ƒğŸ“ŒğŸ’¡â¤ï¸ğŸ”¥âœ¨ã€‚æ‰‹ç»˜ä¸‹åˆ’çº¿å¼ºè°ƒâ€œç»“æ„â€ã€‚åº•éƒ¨å°ç­¾ï¼šè¾°è¾°è€å¸ˆã€‚æ•´ä½“é«˜å¯¹æ¯”æ‰‹å¸æ¶‚é¸¦æ„Ÿå†²åˆºæ°›å›´ã€‚",
  "raw": "ä¸»æ ‡é¢˜ï¼šä¸‡èƒ½ç»“æ„é€†è¢­\nå°é¢æè¿°ï¼šäº®é»„åº•ï¼Œä¸­å¿ƒå¤§å·æ‰‹å†™ä¸»æ ‡é¢˜ç™½å­—çº¢æè¾¹å¾®å€¾ï¼Œå‘¨å›´ç²‰è“æ¶‚é¸¦äº‘ã€‚å·¦ä¸Šçº¢ä¾¿ç­¾ï¼šæœŸä¸­æœ€å1ä¸ªæœˆï¼›å³ä¸Šè“åœ†æ¡†ï¼šå…ˆæŠ¢ç»“æ„åˆ†ï¼›ä¸‹æ–¹Så½¢ç®­å¤´ä¸‰æ­¥ï¼šâ‘ å®¡é¢˜æŠ“ç‚¹ â‘¡ä¸‡èƒ½ç»“æ„æ­éª¨æ¶ â‘¢å¼€å¤´ç»“å°¾+ç»†èŠ‚å‘¨æ”»ã€‚å·¦ä¸‹æ©™åœˆï¼šå‘Šåˆ«æµæ°´è´¦ã€‚å³ä¸‹ç²‰ç«ç„°+âœ¨ï¼šç¨³æ­¥æåˆ†ã€‚æ•£å¸ƒğŸ“ŒğŸ’¡â¤ï¸ğŸ”¥âœ¨ã€‚æ‰‹ç»˜ä¸‹åˆ’çº¿å¼ºè°ƒâ€œç»“æ„â€ã€‚åº•éƒ¨å°ç­¾ï¼šè¾°è¾°è€å¸ˆã€‚æ•´ä½“é«˜å¯¹æ¯”æ‰‹å¸æ¶‚é¸¦æ„Ÿå†²åˆºæ°›å›´ã€‚"
}
</Json serialized context object>
"""

    tools = [
  {
    "type": "function",
    "function": {
      "name": "generate_python_code",
      "description": "Generate Python code for the process_step function",
      "parameters": {
        "type": "object",
        "properties": {
          "python_code": {
            "type": "string",
            "description": "Complete Python function definition for function `process_step` that performs the requested task"
          }
        },
        "required": [
          "python_code"
        ]
      }
    }
  }
]

    # Execute real call (streaming internally). We allow either native tool_calls or fallback XML parse.
    result = await tool.execute({"prompt": long_prompt, "tools": tools, "max_tokens": 4000})

    # If no native tool_calls, try fallback parse manually (model might have emitted raw XML form)
    if not result.get("tool_calls"):
        parsed = tool.parse_tool_call_from_content(result.get("content", ""), tools)
        if parsed:
            result["tool_calls"] = parsed

    assert result.get("tool_calls"), "Expected at least one tool call (native or fallback parsed) in REAL mode long prompt test"
    assert result["tool_calls"][0]["name"] == "generate_python_code"
    assert result["tool_calls"][0]["arguments"]["python_code"].startswith("def process_step"), "Parsed python_code argument missing or incorrect"
    # Compile to ensure valid code
    code_str = result["tool_calls"][0]["arguments"]["python_code"]
    try:
        compiled = compile(code_str, "<string>", "exec")
    except SyntaxError as e:
        pytest.fail(f"Generated python_code has syntax error: {e}")